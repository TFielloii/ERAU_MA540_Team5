{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module, ModuleList, Linear, Dropout, LayerNorm, Identity, Parameter, init\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTLite(nn.Module):\n",
    "    def __init__(self,\n",
    "                 img_size=224,\n",
    "                 embedding_dim=768,\n",
    "                 n_input_channels=3,\n",
    "                 kernel_size=16,\n",
    "                 dropout=0.,\n",
    "                 attention_dropout=0.1,\n",
    "                 stochastic_depth=0.1,\n",
    "                 num_layers=14,\n",
    "                 num_heads=6,\n",
    "                 mlp_ratio=4.0,\n",
    "                 num_classes=1000,\n",
    "                 positional_embedding='learnable',\n",
    "                 *args, **kwargs):\n",
    "        super(ViTLite, self).__init__()\n",
    "        assert img_size % kernel_size == 0, f\"Image size ({img_size}) has to be\" \\\n",
    "                                            f\"divisible by patch size ({kernel_size})\"\n",
    "        self.tokenizer = Tokenizer(n_input_channels=n_input_channels,\n",
    "                                   n_output_channels=embedding_dim,\n",
    "                                   kernel_size=kernel_size,\n",
    "                                   stride=kernel_size,\n",
    "                                   padding=0,\n",
    "                                   max_pool=False,\n",
    "                                   activation=None,\n",
    "                                   n_conv_layers=1,\n",
    "                                   conv_bias=True)\n",
    "\n",
    "        self.classifier = TransformerClassifier(\n",
    "            sequence_length=self.tokenizer.sequence_length(n_channels=n_input_channels,\n",
    "                                                           height=img_size,\n",
    "                                                           width=img_size),\n",
    "            embedding_dim=embedding_dim,\n",
    "            seq_pool=False,\n",
    "            dropout=dropout,\n",
    "            attention_dropout=attention_dropout,\n",
    "            stochastic_depth=stochastic_depth,\n",
    "            num_layers=num_layers,\n",
    "            num_heads=num_heads,\n",
    "            mlp_ratio=mlp_ratio,\n",
    "            num_classes=num_classes,\n",
    "            positional_embedding=positional_embedding\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tokenizer(x)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 kernel_size, stride, padding,\n",
    "                 pooling_kernel_size=3, pooling_stride=2, pooling_padding=1,\n",
    "                 n_conv_layers=1,\n",
    "                 n_input_channels=3,\n",
    "                 n_output_channels=64,\n",
    "                 in_planes=64,\n",
    "                 activation=None,\n",
    "                 max_pool=True,\n",
    "                 conv_bias=False):\n",
    "        super(Tokenizer, self).__init__()\n",
    "\n",
    "        n_filter_list = [n_input_channels] + \\\n",
    "                        [in_planes for _ in range(n_conv_layers - 1)] + \\\n",
    "                        [n_output_channels]\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            *[nn.Sequential(\n",
    "                nn.Conv2d(n_filter_list[i], n_filter_list[i + 1],\n",
    "                          kernel_size=(kernel_size, kernel_size),\n",
    "                          stride=(stride, stride),\n",
    "                          padding=(padding, padding), bias=conv_bias),\n",
    "                nn.Identity() if activation is None else activation(),\n",
    "                nn.MaxPool2d(kernel_size=pooling_kernel_size,\n",
    "                             stride=pooling_stride,\n",
    "                             padding=pooling_padding) if max_pool else nn.Identity()\n",
    "            )\n",
    "                for i in range(n_conv_layers)\n",
    "            ])\n",
    "\n",
    "        self.flattener = nn.Flatten(2, 3)\n",
    "        self.apply(self.init_weight)\n",
    "\n",
    "    def sequence_length(self, n_channels=3, height=224, width=224):\n",
    "        return self.forward(torch.zeros((1, n_channels, height, width))).shape[1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.flattener(self.conv_layers(x)).transpose(-2, -1)\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weight(m):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Module):\n",
    "    \"\"\"\n",
    "    Obtained from timm: github.com:rwightman/pytorch-image-models\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, num_heads=8, attention_dropout=0.1, projection_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // self.num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = Linear(dim, dim * 3, bias=False)\n",
    "        self.attn_drop = Dropout(attention_dropout)\n",
    "        self.proj = Linear(dim, dim)\n",
    "        self.proj_drop = Dropout(projection_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(Module):\n",
    "    def __init__(self,\n",
    "                 seq_pool=True,\n",
    "                 embedding_dim=768,\n",
    "                 num_layers=12,\n",
    "                 num_heads=12,\n",
    "                 mlp_ratio=4.0,\n",
    "                 num_classes=1000,\n",
    "                 dropout=0.1,\n",
    "                 attention_dropout=0.1,\n",
    "                 stochastic_depth=0.1,\n",
    "                 positional_embedding='learnable',\n",
    "                 sequence_length=None):\n",
    "        super().__init__()\n",
    "        positional_embedding = positional_embedding if \\\n",
    "            positional_embedding in ['sine', 'learnable', 'none'] else 'sine'\n",
    "        dim_feedforward = int(embedding_dim * mlp_ratio)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        self.seq_pool = seq_pool\n",
    "        self.num_tokens = 0\n",
    "\n",
    "        assert sequence_length is not None or positional_embedding == 'none', \\\n",
    "            f\"Positional embedding is set to {positional_embedding} and\" \\\n",
    "            f\" the sequence length was not specified.\"\n",
    "\n",
    "        if not seq_pool:\n",
    "            sequence_length += 1\n",
    "            self.class_emb = Parameter(torch.zeros(1, 1, self.embedding_dim),\n",
    "                                       requires_grad=True)\n",
    "            self.num_tokens = 1\n",
    "        else:\n",
    "            self.attention_pool = Linear(self.embedding_dim, 1)\n",
    "\n",
    "        if positional_embedding != 'none':\n",
    "            if positional_embedding == 'learnable':\n",
    "                self.positional_emb = Parameter(torch.zeros(1, sequence_length, embedding_dim),\n",
    "                                                requires_grad=True)\n",
    "                init.trunc_normal_(self.positional_emb, std=0.2)\n",
    "            else:\n",
    "                self.positional_emb = Parameter(self.sinusoidal_embedding(sequence_length, embedding_dim),\n",
    "                                                requires_grad=False)\n",
    "        else:\n",
    "            self.positional_emb = None\n",
    "\n",
    "        self.dropout = Dropout(p=dropout)\n",
    "        dpr = [x.item() for x in torch.linspace(0, stochastic_depth, num_layers)]\n",
    "        self.blocks = ModuleList([\n",
    "            TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads,\n",
    "                                    dim_feedforward=dim_feedforward, dropout=dropout,\n",
    "                                    attention_dropout=attention_dropout, drop_path_rate=dpr[i])\n",
    "            for i in range(num_layers)])\n",
    "        self.norm = LayerNorm(embedding_dim)\n",
    "\n",
    "        self.fc = Linear(embedding_dim, num_classes)\n",
    "        self.apply(self.init_weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.positional_emb is None and x.size(1) < self.sequence_length:\n",
    "            x = F.pad(x, (0, 0, 0, self.n_channels - x.size(1)), mode='constant', value=0)\n",
    "\n",
    "        if not self.seq_pool:\n",
    "            cls_token = self.class_emb.expand(x.shape[0], -1, -1)\n",
    "            x = torch.cat((cls_token, x), dim=1)\n",
    "\n",
    "        if self.positional_emb is not None:\n",
    "            x += self.positional_emb\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if self.seq_pool:\n",
    "            x = torch.matmul(F.softmax(self.attention_pool(x), dim=1).transpose(-1, -2), x).squeeze(-2)\n",
    "        else:\n",
    "            x = x[:, 0]\n",
    "\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weight(m):\n",
    "        if isinstance(m, Linear):\n",
    "            init.trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, Linear) and m.bias is not None:\n",
    "                init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, LayerNorm):\n",
    "            init.constant_(m.bias, 0)\n",
    "            init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @staticmethod\n",
    "    def sinusoidal_embedding(n_channels, dim):\n",
    "        pe = torch.FloatTensor([[p / (10000 ** (2 * (i // 2) / dim)) for i in range(dim)]\n",
    "                                for p in range(n_channels)])\n",
    "        pe[:, 0::2] = torch.sin(pe[:, 0::2])\n",
    "        pe[:, 1::2] = torch.cos(pe[:, 1::2])\n",
    "        return pe.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(Module):\n",
    "    \"\"\"\n",
    "    Inspired by torch.nn.TransformerEncoderLayer and timm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n",
    "                 attention_dropout=0.1, drop_path_rate=0.1):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.pre_norm = LayerNorm(d_model)\n",
    "        self.self_attn = Attention(dim=d_model, num_heads=nhead,\n",
    "                                   attention_dropout=attention_dropout, projection_dropout=dropout)\n",
    "\n",
    "        self.linear1 = Linear(d_model, dim_feedforward)\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.linear2 = Linear(dim_feedforward, d_model)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else Identity()\n",
    "\n",
    "        self.activation = F.gelu\n",
    "\n",
    "    def forward(self, src: torch.Tensor, *args, **kwargs) -> torch.Tensor:\n",
    "        src = src + self.drop_path(self.self_attn(self.pre_norm(src)))\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.linear2(self.dropout1(self.activation(self.linear1(src))))\n",
    "        src = src + self.drop_path(self.dropout2(src2))\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
    "    \"\"\"\n",
    "    Obtained from: github.com:rwightman/pytorch-image-models\n",
    "    Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
    "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
    "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
    "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
    "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
    "    'survival rate' as the argument.\n",
    "    \"\"\"\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_()  # binarize\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output\n",
    "    \n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"\n",
    "    Obtained from: github.com:rwightman/pytorch-image-models\n",
    "    Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "  def __init__(self,\n",
    "               *args,\n",
    "               **kwargs,\n",
    "              ):\n",
    "    super().__init__()\n",
    "    ...\n",
    "    self.attention_pool = nn.Linear(self.embedding_dim, 1)\n",
    "    ...\n",
    "          \n",
    "  def forward(self, x):\n",
    "    ...\n",
    "    x = torch.matmul(F.softmax(self.attention_pool(x), dim=1).transpose(-1, -2), x).squeeze(-2)\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 kernel_size, stride, padding,\n",
    "                 pooling_kernel_size=3, pooling_stride=2, pooling_padding=1,\n",
    "                 n_conv_layers=1,\n",
    "                 n_input_channels=3,\n",
    "                 n_output_channels=64,\n",
    "                 in_planes=64,\n",
    "                 ):\n",
    "        super(Tokenizer, self).__init__()\n",
    "\n",
    "        n_filter_list = [n_input_channels] + \\\n",
    "                        [in_planes for _ in range(n_conv_layers - 1)] + \\\n",
    "                        [n_output_channels]\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            *[nn.Sequential(\n",
    "                nn.Conv2d(n_filter_list[i], n_filter_list[i + 1],\n",
    "                          kernel_size=kernel_size,\n",
    "                          stride=stride,\n",
    "                          padding=padding, \n",
    "                          bias=False),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=pooling_kernel_size,\n",
    "                             stride=pooling_stride,\n",
    "                             padding=pooling_padding)\n",
    "            ) for i in range(n_conv_layers) ])\n",
    "\n",
    "        self.flattener = nn.Flatten(2, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.flattener(self.conv_layers(x)).transpose(-2, -1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a067e5d24d7023c7ea9da287b80a34b0c0bee3c6bc664947e5070c2c6c192d2c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
